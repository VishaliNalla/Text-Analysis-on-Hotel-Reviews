{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import heapq\n",
    "import numpy as np\n",
    "import nltk\n",
    "import io\n",
    "import enchant\n",
    "import sentlex\n",
    "import sentlex.sentanalysis\n",
    "import re\n",
    "import preprocess1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IMPORT AND CORPUS CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel=pd.read_csv(\"Data\\Reviews.csv\")\n",
    "print(hotel.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the imported data frame by HOTEL NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel = hotel.sort_values('name', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split all the reviews into sentences.\n",
    "\n",
    "Run a FOR loop on all rows of the dataset to create one txt file for each row with the data from the Review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['name','review_sent']\n",
    "hotel_sents_df = pd.DataFrame(index=list(range(0,500000)), columns=columns)\n",
    "#hotel_sents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "hotel['review'].fillna(\" \", inplace=True)\n",
    "nrows = len(hotel)-1\n",
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,nrows):\n",
    "    hotel_sents= []\n",
    "    hotel_sents=nltk.tokenize.sent_tokenize(hotel['review'][i])\n",
    "    j=0\n",
    "    for j in range(0,len(hotel_sents)):\n",
    "        hotel_sents_df['name'][k] = hotel['name'][i]\n",
    "        hotel_sents_df['review_sent'][k] = hotel_sents[j]\n",
    "        j+=1\n",
    "        k+=1\n",
    "    i+=1\n",
    "    #print('i',i)\n",
    "    #print('j',j)\n",
    "    #print('k',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_sents_df.dropna(subset=['name'], how='all', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=1\n",
    "hold_hotel = ' '\n",
    "len(hotel_sents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(hotel_sents_df)-1):\n",
    "    if  hold_hotel != hotel_sents_df['name'][i]:\n",
    "        j=1 \n",
    "        hold_hotel = hotel_sents_df['name'][i]\n",
    "    f = open(str(hotel_sents_df['name'][i])+\" - \"+str(j)+'.txt', 'w+')\n",
    "    f.write(str(hotel_sents_df['review_sent'][i]))\n",
    "    f.close()\n",
    "    #print(i)\n",
    "    i+=1\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All the text files will be generated where ipynb file is placed. \n",
    "\n",
    "Create a folder called Hotel Corpus Sents and move all text files into this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the created corpus folder of 100k review sentence files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_corpus = preprocess1.load_corpus('./Hotel_Corpus_Sents')\n",
    "hotel_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the pre processing steps on the corpus:\n",
    "    \n",
    "    1. Tokenization\n",
    "    2. Case conversion to lower\n",
    "    3. Removal of non alphabetic characters\n",
    "    4. Stop word removal\n",
    "    5. Non English words removal\n",
    "    6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "new_stop_words = ['hotel','room','negative','good','great','love','recommend','grove']\n",
    "for i in range(0,len(new_stop_words)):\n",
    "    stop_list.append(new_stop_words[i])\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = hotel_corpus.fileids()\n",
    "docs1 = []\n",
    "for fid in fids:\n",
    "    doc_raw = hotel_corpus.raw(fid)\n",
    "    doc = nltk.word_tokenize(doc_raw)\n",
    "    docs1.append(doc)\n",
    "docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "docs5 = [[w for w in doc if d.check(w)] for doc in docs4]\n",
    "hotel_docs = [[stemmer.stem(w) for w in doc] for doc in docs5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the corpus dictionary.\n",
    "2. Create the document sparse matrix representations for all the documents in the corpus.\n",
    "3. Store the file ids of the corpus documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_dictionary = gensim.corpora.Dictionary(hotel_docs)\n",
    "hotel_vecs = preprocess1.docs2vecs(hotel_docs, hotel_dictionary)\n",
    "fids = hotel_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the number of files in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATENT DIRICHLET ALLOCATION - TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LDA for topic modelling with number of topics to identify = 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_lda = gensim.models.ldamodel.LdaModel(corpus=hotel_vecs, id2word=hotel_dictionary, num_topics=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the top 20 words by TOPIC-WORD distribution for each identified topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = hotel_lda.show_topics(7, 10)\n",
    "\n",
    "for i in range(0, 7):\n",
    "    print(topics[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the generated LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_lda.save('lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the saved model and view the top 20 words of each topic to validate the consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load('lda.model')\n",
    "model.show_topics(7,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a null dataframe with N rows where N is the number of documents in the corpus to store the DOCUMENT-TOPIC distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['File_ID']\n",
    "Doc_Topic_Dist = pd.DataFrame(index=list(range(0,len(fids))), columns=columns)\n",
    "#print(Doc_Topic_Dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and store the document topic distribution for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(fids):\n",
    "    index_of_file = fids.index(fids[i])\n",
    "    vec = hotel_vecs[index_of_file]\n",
    "    vec_lda = model[vec]\n",
    "    #print(vec_lda)\n",
    "    #print(i)\n",
    "    #print (val)\n",
    "    Doc_Topic_Dist.loc[i,\"File_ID\"] = fids[i]\n",
    "    for j,k in vec_lda:\n",
    "        Doc_Topic_Dist.loc[i,j] = vec_lda[j][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns with the topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Topic_Dist.columns = [\"File_ID\",\"Location\",\"Staff\",\"Value For Money\",\"Food\",\"NA\",\"NA2\",\"Amenities\"]\n",
    "print(Doc_Topic_Dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a null dataframe to store the top two topics by distribution for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['File_ID']\n",
    "Doc_Topic_Dist_Top2 = pd.DataFrame(index=list(range(0,len(fids))), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the FILE ID column temporarily to identify the top topics for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Topic_Dist_Sub = Doc_Topic_Dist.iloc[:,1:8]\n",
    "Doc_Topic_Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_corpus.raw(fileids='1785 Inn - 10.txt')\n",
    "hotel_corpus.raw(fileids='1785 Inn - 32.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the topic distributions and pick the top 2 to represent the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(0,len(Doc_Topic_Dist)):\n",
    "    Top_Topics = heapq.nlargest(2, Doc_Topic_Dist_Sub.iloc[t])\n",
    "    Doc_Topic_Dist_Top2.loc[t,1] = Top_Topics[0]\n",
    "    Doc_Topic_Dist_Top2.loc[t,2] = Top_Topics[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-attach the FILE ID column for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Topic_Dist_Top2['File_ID'] = Doc_Topic_Dist['File_ID']\n",
    "print(Doc_Topic_Dist_Top2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the labels of the top two topics for each document, create a null dataframe to store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['File_ID','Top1_Topic','Top2_Topic']\n",
    "Doc_Topic_Top2 = pd.DataFrame(index=list(range(0,len(fids))), columns=columns)\n",
    "Doc_Topic_Top2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the FILE ID column temporarily to identify the top topics for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Topic_Dist_Sub2 = Doc_Topic_Dist.iloc[:,1:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the topic labels and pick the top 2 to represent the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(0,len(Doc_Topic_Dist_Sub2)):\n",
    "    Top_Topics = heapq.nlargest(2, Doc_Topic_Dist_Sub2.iloc[t])\n",
    "    Top1_Index = np.where(Doc_Topic_Dist_Sub2.iloc[[t]] == Top_Topics[0])\n",
    "    Top2_Index = np.where(Doc_Topic_Dist_Sub2.iloc[[t]] == Top_Topics[1])\n",
    "    Top1 = Doc_Topic_Dist_Sub2.columns[Top1_Index[1]]\n",
    "    Top2 = Doc_Topic_Dist_Sub2.columns[Top2_Index[1]]\n",
    "    Doc_Topic_Top2.loc[t,\"Top1_Topic\"] = Top1[0]\n",
    "    Doc_Topic_Top2.loc[t,\"Top2_Topic\"] = Top2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-attach the FILE ID column for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Topic_Top2['File_ID'] = Doc_Topic_Dist['File_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Doc_Topic_Top2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWN = sentlex.SWN3Lexicon()\n",
    "classifier = sentlex.sentanalysis.BasicDocSentiScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['File_ID','Positive Score','Negative Score']\n",
    "Doc_Senti = pd.DataFrame(index=list(range(0,len(fids))), columns=columns)\n",
    "classifier_result=(0,0)\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fil in fids:\n",
    "    classifier_result=classifier.classify_document(hotel_corpus.raw(fileids=fil), tagged=False, L=SWN, a=True, v=True, n=False, r=False, negation=True, verbose=False)\n",
    "    Doc_Senti['File_ID'][idx] = fil\n",
    "    Doc_Senti['Positive Score'][idx] = classifier_result[0]\n",
    "    Doc_Senti['Negative Score'][idx] = classifier_result[1]\n",
    "    idx+=1\n",
    "    #print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Doc_Sent[Doc_Sent['Positive Score'] < Doc_Sent['Negative Score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
